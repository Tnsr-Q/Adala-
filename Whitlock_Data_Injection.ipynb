{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tnsr-Q/Adala-/blob/main/Whitlock_Data_Injection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course. I have rewritten the content in the style of the first response, avoiding the previous format and optimizing it for clarity, professional presentation, and use within environments like Google Colab.\n",
        "\n",
        "Here is the consolidated and professionally formatted guide.\n",
        "\n",
        "### **Overview**\n",
        "\n",
        "This guide outlines a robust data ingestion and indexing pipeline designed for processing media episodes. The workflow covers data preparation, bulk loading into Redis, vector indexing for semantic search via Mem0, and essential verification checks. Advanced techniques for handling large-scale datasets and ensuring data integrity are also provided. The code snippets are optimized for use in environments like Google Colab, with notes on handling shell commands and connecting to services.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Step 1: Prepare Your Data Files**\n",
        "\n",
        "First, we'll transform the source data into formats optimized for bulk loading and analysis. We will use NDJSON for episode data due to its stream-friendly nature and a standard CSV for relational \"mentions\" data.\n",
        "\n",
        "#### **1.1 Format Episodes as NDJSON**\n",
        "\n",
        "Convert your existing episode JSON files into Newline Delimited JSON (NDJSON), where each line is a self-contained JSON object.\n",
        "\n",
        "\\<details\\>\n",
        "\\<summary\\>\\<strong\\>Google Colab Note\\</strong\\>\\</summary\\>\n",
        "To run shell commands in Colab, prefix them with `!`. You'll first need to upload your data or mount your Google Drive.\n",
        "\\</details\\>\n",
        "\n",
        "```bash\n",
        "# In your data directory, combine all JSON files into a single NDJSON file.\n",
        "# The `jq -c .` command compacts each JSON object into a single line.\n",
        "!jq -c . *.json > episodes.ndjson\n",
        "```\n",
        "\n",
        "#### **1.2 Create Celebrity Mentions CSV**\n",
        "\n",
        "Generate a CSV file that maps celebrity names to the episodes in which they are mentioned, including a timestamp and category.\n",
        "\n",
        "```bash\n",
        "# Create the header row for the CSV file.\n",
        "!echo \"name,episode_id,timestamp,category\" > mentions.csv\n",
        "\n",
        "# Append mentions by parsing the NDJSON file.\n",
        "# This example extracts main topics and categorizes them as 'sports'.\n",
        "!jq -r '.video_metadata.video_id as $id | .content_analysis.main_topics[] | \"\\(.),\\($id),\\(now | strftime(\"%Y-%m-%dT%H:%M:%SZ\")),sports\"' episodes.ndjson >> mentions.csv\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### **Step 2: One-Time Redis Bulk Load**\n",
        "\n",
        "With the data prepared, we will perform a highly efficient bulk load into Redis using its pipe mode. This minimizes network latency by sending multiple commands in a single operation.\n",
        "\n",
        "#### **2.1 Install Redis Tools & Connect from Colab**\n",
        "\n",
        "You'll need `redis-cli`, which is part of the `redis-tools` package.\n",
        "\n",
        "```bash\n",
        "# Install redis-tools in the Colab environment\n",
        "!sudo apt-get install redis-tools -y\n",
        "!pip install redis\n",
        "```\n",
        "\n",
        "\\<details\\>\n",
        "\\<summary\\>\\<strong\\>Connecting to a Remote Redis from Colab\\</strong\\>\\</summary\\>\n",
        "When running commands like `redis-cli`, you must specify the host and port of your remote Redis instance (e.g., from Redis Cloud or Aiven).\n",
        "\n",
        "Example: `!redis-cli -h <your-redis-host> -p <port> -a <password> --pipe`\n",
        "\n",
        "\\</details\\>\n",
        "\n",
        "#### **2.2 Load Episode Data & Mentions**\n",
        "\n",
        "Use the pipe mode for a high-throughput atomic load.\n",
        "\n",
        "```bash\n",
        "# 1. Load all episodes. Assumes each line is a valid Redis command (e.g., SET key value).\n",
        "# Replace with your Redis connection details.\n",
        "!cat episodes.ndjson | redis-cli -h <host> -p <port> -a <password> --pipe\n",
        "\n",
        "# 2. Load celebrity mentions into a Redis Sorted Set.\n",
        "# This allows for powerful time-based querying.\n",
        "!awk -F, '{print \"ZADD person:\"$1\":episodes \"$3\" \"$2}' mentions.csv | redis-cli -h <host> -p <port> -a <password> --pipe\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### **Step 3: Automated Vector Indexing with Mem0**\n",
        "\n",
        "Next, we will index the episode content with Mem0 to enable powerful semantic search capabilities. This Python script iterates through our newly loaded Redis keys, extracts relevant data, and sends it to the Mem0 indexing service.\n",
        "\n",
        "#### **`mem0_indexer.py`**"
      ],
      "metadata": {
        "id": "46FDXsMHTqqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BUsu0DruUQCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import json\n",
        "import os\n",
        "import redis\n",
        "from mem0 import Mem0Client\n",
        "\n",
        "# --- Configuration ---\n",
        "# It's best practice to use environment variables for credentials.\n",
        "# In Colab, you can set these using the 'Secrets' tab.\n",
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
        "REDIS_PORT = int(os.getenv(\"REDIS_PORT\", 6379))\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\")\n",
        "MEM0_API_KEY = os.getenv(\"MEM0_API_KEY\")\n",
        "\n",
        "# --- Client Initialization ---\n",
        "try:\n",
        "    r = redis.Redis(\n",
        "        host=REDIS_HOST,\n",
        "        port=REDIS_PORT,\n",
        "        password=REDIS_PASSWORD,\n",
        "        decode_responses=True # Simplifies handling of keys/values\n",
        "    )\n",
        "    r.ping() # Verify connection\n",
        "    print(\"Successfully connected to Redis.\")\n",
        "except redis.exceptions.ConnectionError as e:\n",
        "    print(f\"Error connecting to Redis: {e}\")\n",
        "    # Exit or handle error appropriately\n",
        "    exit()\n",
        "\n",
        "mem0 = Mem0Client(api_key=MEMO_API_KEY)\n",
        "\n",
        "async def index_episodes():\n",
        "    \"\"\"\n",
        "    Scans for legacy episode keys in Redis, extracts metadata,\n",
        "    and indexes it as a vector in Mem0.\n",
        "    \"\"\"\n",
        "    print(\"Starting episode indexing...\")\n",
        "    keys_processed = 0\n",
        "    # Use scan_iter for memory-efficient iteration over large key sets\n",
        "    for key in r.scan_iter(\"episode:*:legacy\"):\n",
        "        try:\n",
        "            data = json.loads(r.hget(key, \"raw\"))\n",
        "\n",
        "            # Extract features for vectoring and metadata with safe .get() calls\n",
        "            main_topics_vector = data.get(\"content_analysis\", {}).get(\"main_topics\", [])\n",
        "            video_meta = data.get(\"video_metadata\", {})\n",
        "            whitlock_tech = data.get(\"whitlock_techniques\", {})\n",
        "\n",
        "            controversy_elements = whitlock_tech.get(\"controversy_elements\", [])\n",
        "            spice_level = max(c.get(\"controversy_level\", 0) for c in controversy_elements) if controversy_elements else 0\n",
        "\n",
        "            await mem0.add(\n",
        "                id=key, # Use a deterministic ID\n",
        "                text=\", \".join(main_topics_vector), # Text to be embedded by Mem0\n",
        "                metadata={\n",
        "                    \"duration\": video_meta.get(\"duration_seconds\"),\n",
        "                    \"spice_score\": spice_level\n",
        "                }\n",
        "            )\n",
        "            keys_processed += 1\n",
        "            if keys_processed % 100 == 0:\n",
        "                print(f\"Indexed {keys_processed} vectors...\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Skipping key {key}: Invalid JSON in 'raw' field.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing key {key}: {e}\")\n",
        "\n",
        "    print(f\"Finished indexing. Total vectors processed: {keys_processed}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(index_episodes())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RMSQxqbxTqqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Run the Indexer**\n",
        "\n",
        "```bash\n",
        "!python mem0_indexer.py\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### **Step 4: Verification and Quick Checks**\n",
        "\n",
        "Perform these quick \"smoke tests\" to validate that the data has been loaded and indexed correctly.\n",
        "\n",
        "#### **4.1 Check Redis**\n",
        "\n",
        "Confirm that the keys have been created and populated.\n",
        "\n",
        "```bash\n",
        "# Get a summary of large keys, filtering for our episode data\n",
        "!redis-cli -h <host> -p <port> -a <password> --bigkeys | grep \"episode\"\n",
        "\n",
        "# Check the cardinality (count) of a specific celebrity's sorted set.\n",
        "!redis-cli -h <host> -p <port> -a <password> ZCARD \"person:Deion Sanders:episodes\"\n",
        "```\n",
        "\n",
        "#### **4.2 Check Mem0**\n",
        "\n",
        "Query the Mem0 API to ensure your vectors have been indexed and are searchable.\n",
        "\n",
        "\\<details\\>\n",
        "\\<summary\\>\\<strong\\>Setting API Key in Colab\\</strong\\>\\</summary\\>\n",
        "Store your `MEM0_KEY` in Colab's secrets manager for security. In the `curl` command, you would access it as `$MEM0_KEY`.\n",
        "\\</details\\>\n",
        "\n",
        "```bash\n",
        "# Example search query for episodes about the \"NFL\"\n",
        "!curl -X POST \"https://api.mem0.ai/v1/search\" \\\n",
        "     -H \"Content-Type: application/json\" \\\n",
        "     -H \"Authorization: Bearer $MEM0_KEY\" \\\n",
        "     -d '{\n",
        "         \"query\": \"episodes about the NFL\",\n",
        "         \"limit\": 5\n",
        "     }'\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### **Critical Pro Tips for Scaling**\n",
        "\n",
        "#### **Handling 500+ Episodes with Parallel Ingestion**\n",
        "\n",
        "For very large datasets, split the data into chunks and upload them in parallel.\n",
        "\n",
        "```bash\n",
        "# Step 1: Split the master NDJSON file into smaller chunks of 100 lines each\n",
        "!split -l 100 episodes.ndjson episodes_chunk_\n",
        "\n",
        "# Step 2: Use GNU Parallel to upload chunks concurrently (install if needed)\n",
        "!sudo apt-get install parallel -y\n",
        "!parallel -j 4 \"cat {} | redis-cli -h <host> -p <port> -a <password> --pipe\" ::: episodes_chunk_*\n",
        "```\n",
        "\n",
        "#### **Automated Schema Migration and Enforcement**\n",
        "\n",
        "To maintain data consistency, you can manage data schemas programmatically. This example shows how to enforce a schema in Google Firestore.\n",
        "\n",
        "##### **`schema_enforcer.py`**"
      ],
      "metadata": {
        "id": "27safcMbTqqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import firestore\n",
        "from google.cloud.firestore import SERVER_TIMESTAMP\n",
        "\n",
        "# Define the expected fields for your collections.\n",
        "# Best practice: manage this in a separate config file.\n",
        "SCHEMA_MAP = {\n",
        "    \"episodes\": [\"title\", \"air_date\", \"video_id\", \"source_url\"],\n",
        "    \"people\": [\"name\", \"category\", \"first_seen\"],\n",
        "}\n",
        "\n",
        "def enforce_schemas():\n",
        "    \"\"\"\n",
        "    Writes a schema document to a Firestore `_schemas` collection.\n",
        "    This can be used by security rules or backend services to validate documents.\n",
        "    \"\"\"\n",
        "    # In Colab, you would authenticate first:\n",
        "    # from google.colab import auth\n",
        "    # auth.authenticate_user()\n",
        "    db = firestore.Client()\n",
        "    print(\"Enforcing Firestore schemas...\")\n",
        "\n",
        "    for collection_name, fields in SCHEMA_MAP.items():\n",
        "        schema_doc_ref = db.document(f\"_schemas/{collection_name}\")\n",
        "        schema_data = {\n",
        "            \"fields\": {field_name: {\"type\": \"string\", \"required\": True} for field_name in fields},\n",
        "            \"last_updated\": SERVER_TIMESTAMP\n",
        "        }\n",
        "        schema_doc_ref.set(schema_data)\n",
        "        print(f\"  - Schema for '{collection_name}' has been written.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    enforce_schemas()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "AdG6DW1CTqqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### **Data Architecture Schemas**\n",
        "\n",
        "The following sections define the core data structures used across the system, from the detailed episode analysis to the schemas for Redis and the Reinforcement Learning environment.\n",
        "\n",
        "#### **Core Episode Processing Schema**\n",
        "\n",
        "This is the canonical JSON structure for a single, fully analyzed episode.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"video_metadata\": {\n",
        "    \"video_id\": \"string\",\n",
        "    \"title\": \"string\",\n",
        "    \"upload_date\": \"datetime_iso8601\",\n",
        "    \"duration_seconds\": \"integer\"\n",
        "  },\n",
        "  \"content_analysis\": {\n",
        "    \"main_topics\": [\"string\"],\n",
        "    \"primary_thesis\": \"string\",\n",
        "    \"argument_structure\": {\n",
        "      \"opening_hook\": \"string\",\n",
        "      \"evidence_presented\": [\"string\"],\n",
        "      \"conclusion\": \"string\"\n",
        "    }\n",
        "  },\n",
        "  \"whitlock_techniques\": {\n",
        "    \"signature_phrases\": [\"string\"],\n",
        "    \"controversy_elements\": [\n",
        "      {\n",
        "        \"spicy_quote\": \"string\",\n",
        "        \"controversy_level\": \"float\",\n",
        "        \"timestamp\": \"string\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"performance_analytics\": {\n",
        "    \"engagement_rate\": \"float\",\n",
        "    \"comment_sentiment\": \"float\",\n",
        "    \"viral_potential\": \"float\"\n",
        "  },\n",
        "  \"mobile_optimization\": {\n",
        "    \"clip_recommendations\": [\n",
        "      {\n",
        "        \"start_time\": \"string\",\n",
        "        \"end_time\": \"string\",\n",
        "        \"clip_title\": \"string\",\n",
        "        \"viral_potential\": \"float\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Redis Schema Architecture**\n",
        "\n",
        "This defines how data is structured within Redis for high-performance access."
      ],
      "metadata": {
        "id": "pNQkTkokTqqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python dictionary serves as a reference for your Redis key structures.\n",
        "\n",
        "REDIS_SCHEMAS = {\n",
        "    \"episode_data\": {\n",
        "        \"key_format\": \"episode:{video_id}:legacy\",\n",
        "        \"type\": \"Hash\",\n",
        "        \"fields\": {\n",
        "            \"raw\": \"The full JSON object of the episode, compressed (e.g., gzip).\",\n",
        "            \"timestamp\": \"ISO 8601 timestamp for time-based filtering.\"\n",
        "        }\n",
        "    },\n",
        "    \"person_mentions\": {\n",
        "        \"key_format\": \"person:{normalized_name}:episodes\",\n",
        "        \"type\": \"Sorted Set (ZSET)\",\n",
        "        \"description\": \"Stores episode IDs where a person is mentioned.\",\n",
        "        \"score\": \"Unix timestamp of the mention.\",\n",
        "        \"value\": \"episode_id\"\n",
        "    },\n",
        "    \"historical_stats\": {\n",
        "        \"key_format\": \"whitlock:stats:{period}\", # e.g., period = \"90d\"\n",
        "        \"type\": \"Hash\",\n",
        "        \"fields\": {\n",
        "            \"avg_spice_score\": \"float\",\n",
        "            \"top_topics\": \"JSON string array\",\n",
        "            \"top_mentions\": \"JSON string array\"\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-ftrsQE-TqqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Mem0 Indexing & RL Environment Schema**\n",
        "\n",
        "This defines the structure for Mem0, covering both semantic search indexing and the Reinforcement Learning environment's configuration."
      ],
      "metadata": {
        "id": "0gWiCagSTqqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This file outlines the schemas for Mem0.\n",
        "\n",
        "# 1. Mem0 Reinforcement Learning (RL) Environment Schema\n",
        "# This dictionary is stored as metadata against a single vector in Mem0.\n",
        "RL_ENVIRONMENT_SCHEMA = {\n",
        "    \"state_space\": {\n",
        "        \"description\": \"Defines the inputs the agent observes.\",\n",
        "        \"current_episode_content\": \"embedding:768\",\n",
        "        \"live_audience_feedback\": \"embedding:256\",\n",
        "        \"moral_alignment_score\": \"float\"\n",
        "    },\n",
        "    \"action_space\": {\n",
        "        \"description\": \"Defines the possible actions the agent can take.\",\n",
        "        \"actions\": [\n",
        "            {\"name\": \"pivot_topic\", \"params\": {\"target_topic\": \"string\", \"intensity\": \"float\"}},\n",
        "            {\"name\": \"introduce_rhetorical_device\", \"params\": {\"device_type\": \"enum\"}}\n",
        "        ]\n",
        "    },\n",
        "    \"reward_components\": {\n",
        "        \"description\": \"Defines the sources and weights for the reward signal.\",\n",
        "        \"engagement\": {\"weight\": 0.6, \"source\": \"redis\"},\n",
        "        \"moral_alignment\": {\"weight\": 0.3, \"source\": \"cerebras_api\"},\n",
        "        \"novelty\": {\"weight\": 0.1, \"source\": \"mem0_search\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Example: Storing the RL schema in Mem0\n",
        "async def initialize_rl_env_in_mem0(mem0_client):\n",
        "    \"\"\"Stores the RL environment schema in Mem0 for easy retrieval.\"\"\"\n",
        "    await mem0_client.add(\n",
        "        id=\"rl_env:whitlock_production_v1\",\n",
        "        text=\"Configuration for the Whitlock production RL environment.\",\n",
        "        metadata=RL_ENVIRONMENT_SCHEMA\n",
        "    )\n",
        "    print(\"RL Environment schema successfully stored in Mem0.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xkZtbWAsTqqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Firestore Temporary Article Pipeline**\n",
        "\n",
        "This schema defines the structure for temporary articles managed in Firestore, which pass through a multi-stage verification process."
      ],
      "metadata": {
        "id": "cXbOeGb3TqqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This file defines the schema for the Firestore-based verification pipeline.\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict\n",
        "\n",
        "class Claim(BaseModel):\n",
        "    claim_text: str\n",
        "    verification_status: str = \"unverified\"\n",
        "    cerebras_analysis: Dict = Field(default_factory=dict)\n",
        "\n",
        "class ProcessingStage(BaseModel):\n",
        "    completed: bool = False\n",
        "    approval_status: str = \"pending\"\n",
        "    score: float = 0.0\n",
        "    notes: str = \"\"\n",
        "\n",
        "class FirestoreArticle(BaseModel):\n",
        "    \"\"\"Pydantic model for a document in the 'temp_articles' collection.\"\"\"\n",
        "    raw_content: str\n",
        "    source_episode_id: str\n",
        "    created_at: str # ISO 8601 timestamp\n",
        "    status: str = \"pending_verification\"\n",
        "    extracted_claims: List[Claim] = Field(default_factory=list)\n",
        "    processing_stages: Dict[str, ProcessingStage] = Field(default_factory=dict) # e.g., {\"ppo_eval\": {...}, \"shisa_approval\": {...}}\n",
        "\n",
        "# This Pydantic model can be used in your application to validate data\n",
        "# before writing to Firestore, ensuring data integrity.\n",
        "#\n",
        "# Example Usage:\n",
        "# article_data = FirestoreArticle(raw_content=\"...\", source_episode_id=\"abc-123\", ...)\n",
        "# firestore_client.collection(\"temp_articles\").add(article_data.model_dump())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CvuWCyBaTqqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/Lightning-AI/lightning\">https://github.com/Lightning-AI/lightning</a> subject to Apache - 2.0</li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "QQ0yjTYOTqqY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}